{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"h1xJhIUTsQQS","executionInfo":{"status":"ok","timestamp":1685693689224,"user_tz":-60,"elapsed":28681,"user":{"displayName":"Mohamed Hedi KHEMIRI","userId":"10636555394238233128"}},"outputId":"453f7c16-346f-4d39-9158-dd3e06e75b98","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4fmA1sNO1C0_","executionInfo":{"status":"ok","timestamp":1685693703615,"user_tz":-60,"elapsed":14403,"user":{"displayName":"Mohamed Hedi KHEMIRI","userId":"10636555394238233128"}},"outputId":"f93b772c-c817-4510-ecf0-a2f1dfebdd56"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.15.1 tokenizers-0.13.3 transformers-4.29.2\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dXAzmZ3ooZWa"},"outputs":[],"source":["import transformers\n","from sklearn.metrics import classification_report, confusion_matrix\n","import pandas as pd\n","import tensorflow as tf\n","import pandas as pd\n","import string\n","import re\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from transformers import BertForSequenceClassification\n","\n","from transformers import TFBertModel\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","import numpy as np\n","\n","import tensorflow_hub as hub\n"]},{"cell_type":"code","source":["train_df = pd.read_csv(\"/content/drive/MyDrive/PFA/DATASET/trainset.csv\",encoding='ISO-8859-1')\n","valid_df = pd.read_csv(\"/content/drive/MyDrive/PFA/DATASET/validationset.csv\",encoding='ISO-8859-1')\n","test_df =pd.read_csv(\"/content/drive/MyDrive/PFA/DATASET/testset.csv\",encoding='ISO-8859-1')"],"metadata":{"id":"sWNJ3RFnsYxd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_captions(df,output):\n","  individual_captions = pd.DataFrame(columns=[\"caption\",\"I-E\",\"S-N\",\"T-F\",\"J-P\"])\n","\n","  for index,row in df.iterrows():\n","    for caption_index,caption in enumerate(eval(row['text_clean'])):\n","      # Define the new row to be added\n","      new_row = {'caption': caption.strip(\"\\n\"),\"I-E\":df[\"I-E\"][index],\\\n","                 'S-N':df[\"S-N\"][index],'T-F':df[\"T-F\"][index],'J-P':df[\"J-P\"][index]}\n","      # Use the loc method to add the new row to the DataFrame\n","      individual_captions.loc[len(individual_captions)] = new_row\n","  individual_captions =individual_captions.drop(individual_captions[individual_captions['caption']==''].index)\n","  individual_captions.to_csv(output)\n","  return individual_captions\n","  "],"metadata":{"id":"1kaHi6UPsPwX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df_indiv = extract_captions(train_df,\"/content/drive/MyDrive/PFA/DATASET/indiv_trainset.csv\")"],"metadata":{"id":"e9bz5tJ4tKb-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["valid_df_indiv  = extract_captions(valid_df,\"/content/drive/MyDrive/PFA/DATASET/indiv_validationset.csv\")"],"metadata":{"id":"dTVXOmC2tT5g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df_indiv  = extract_captions(test_df,\"/content/drive/MyDrive/PFA/DATASET/indiv_testset.csv\")"],"metadata":{"id":"lgb6_uHAtUD4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df_indiv = pd.read_csv(\"/content/drive/MyDrive/PFA/DATASET/indiv_trainset.csv\")\n","valid_df_indiv = pd.read_csv(\"/content/drive/MyDrive/PFA/DATASET/indiv_validationset.csv\")\n","test_df_indiv = pd.read_csv(\"/content/drive/MyDrive/PFA/DATASET/indiv_testset.csv\")"],"metadata":{"id":"uCoqdmT8JaBf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def transform_attr(value):\n","    if value == 1:\n","        return [1, 0]\n","    elif value == 0:\n","        return [0, 1]\n","    else:\n","        return []\n","y_train = train_df_indiv['I-E'].apply(lambda x: transform_attr(x))\n","y_train = np.array(y_train.to_list())\n","\n","y_valid = valid_df_indiv['I-E'].apply(lambda x: transform_attr(x))\n","y_valid = np.array(y_valid.to_list())\n","\n","y_test = train_df_indiv['I-E'].apply(lambda x: transform_attr(x))\n","y_test = np.array(y_test.to_list())"],"metadata":{"id":"jY0Nho8S7L4V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["m_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n","bert_layer = hub.KerasLayer(m_url, trainable=True)"],"metadata":{"id":"dshLVEex5-wT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased',do_lower_case =True)"],"metadata":{"id":"fsnHg0id_uLs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def bert_encode(texts, tokenizer, max_len=512):\n","    all_tokens = []\n","    all_masks = []\n","    all_segments = []\n","    \n","    for text in texts:\n","        text = tokenizer.tokenize(text)\n","        \n","        text = text[:max_len-2]\n","        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n","        pad_len = max_len-len(input_sequence)\n","        \n","        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n","        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n","        segment_ids = [0] * max_len\n","        \n","        all_tokens.append(tokens)\n","        all_masks.append(pad_masks)\n","        all_segments.append(segment_ids)\n","        \n","    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"],"metadata":{"id":"GwrNCiv15-yk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras import backend as K\n","def recall_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","\n","def precision_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","def f1_m(y_true, y_pred):\n","    precision = precision_m(y_true, y_pred)\n","    recall = recall_m(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n","def build_model(bert_layer, max_len=512):\n","    \n","\n","    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n","    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n","    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n","    \n","    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n","    \n","    clf_output = sequence_output[:, 0, :]\n","    \n","    lay = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n","    lay = tf.keras.layers.Dropout(0.2)(lay)\n","    lay = tf.keras.layers.Dense(32, activation='relu')(lay)\n","    lay = tf.keras.layers.Dropout(0.2)(lay)\n","    out = tf.keras.layers.Dense(2, activation='sigmoid')(lay)\n","    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n","    model.compile(tf.keras.optimizers.Adam(lr=2e-5), loss='binary_crossentropy', metrics=[\"accuracy\",f1_m,precision_m, recall_m])\n","    \n","    return model\n","    "],"metadata":{"id":"lNDPfBT_5-0d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_len = 512\n","train_input = bert_encode(train_df_indiv.caption.values, tokenizer, max_len=max_len)\n","test_input = bert_encode(test_df_indiv.caption.values, tokenizer, max_len=max_len)\n","train_labels = y_train\n"],"metadata":{"id":"hajiwgzP5-2g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = build_model(bert_layer, max_len=max_len)\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3RhIrEWw6Znk","executionInfo":{"status":"ok","timestamp":1685688402548,"user_tz":-60,"elapsed":42,"user":{"displayName":"PFA team","userId":"12913494494870356669"}},"outputId":"c98b803d-c01c-4671-d5c2-2dd485282288"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_word_ids (InputLayer)    [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_mask (InputLayer)        [(None, 512)]        0           []                               \n","                                                                                                  \n"," segment_ids (InputLayer)       [(None, 512)]        0           []                               \n","                                                                                                  \n"," keras_layer_1 (KerasLayer)     [(None, 768),        109482241   ['input_word_ids[0][0]',         \n","                                 (None, 512, 768)]                'input_mask[0][0]',             \n","                                                                  'segment_ids[0][0]']            \n","                                                                                                  \n"," tf.__operators__.getitem_2 (Sl  (None, 768)         0           ['keras_layer_1[1][1]']          \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," dense_6 (Dense)                (None, 64)           49216       ['tf.__operators__.getitem_2[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dropout_4 (Dropout)            (None, 64)           0           ['dense_6[0][0]']                \n","                                                                                                  \n"," dense_7 (Dense)                (None, 32)           2080        ['dropout_4[0][0]']              \n","                                                                                                  \n"," dropout_5 (Dropout)            (None, 32)           0           ['dense_7[0][0]']                \n","                                                                                                  \n"," dense_8 (Dense)                (None, 2)            66          ['dropout_5[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,533,603\n","Trainable params: 109,533,602\n","Non-trainable params: 1\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n","earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n","\n","train_sh = model.fit(\n","    train_input, train_labels,\n","    validation_split=0.2,\n","    epochs=5,\n","    callbacks=[checkpoint, earlystopping],\n","    batch_size=32,\n","    verbose=1\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"_2IigRpZ6Zpp","outputId":"c2c37ded-af98-45c0-ef81-06d11fbd72e6","executionInfo":{"status":"error","timestamp":1685691058877,"user_tz":-60,"elapsed":20759,"user":{"displayName":"PFA team","userId":"12913494494870356669"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n"]},{"output_type":"error","ename":"ResourceExhaustedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-d16f8a5a8e9a>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mearlystopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m train_sh = model.fit(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'gradients/transformer/layer_3/activation_3/mul_3_grad/Mul_1' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n      yield self.process_one()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 250, in wrapper\n      runner = Runner(ctx_run, result, future, yielded)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 748, in __init__\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-33-d16f8a5a8e9a>\", line 4, in <cell line: 4>\n      train_sh = model.fit(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1054, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer.py\", line 542, in minimize\n      grads_and_vars = self.compute_gradients(loss, var_list, tape)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer.py\", line 275, in compute_gradients\n      grads = tape.gradient(loss, var_list)\nNode: 'gradients/transformer/layer_3/activation_3/mul_3_grad/Mul_1'\nfailed to allocate memory\n\t [[{{node gradients/transformer/layer_3/activation_3/mul_3_grad/Mul_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_117188]"]}]},{"cell_type":"code","source":["y_pred = model.predict(test_input)\n","print(y_pred)"],"metadata":{"id":"wVx1hzR6Awq8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.save('model_ie.h5')"],"metadata":{"id":"gg0uLepiG0i6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Q1o1-UHvKOrp"},"execution_count":null,"outputs":[]}]}