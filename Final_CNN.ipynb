{"cells":[{"cell_type":"code","source":[],"metadata":{"id":"AnZ6xJrvR0Ob"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19797,"status":"ok","timestamp":1685658061318,"user":{"displayName":"amani bchir","userId":"17875292173569037577"},"user_tz":-60},"id":"xhf6YX59MLiD","outputId":"bce02ae1-93c7-4dfc-9da7-92ae7c24fa8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6792,"status":"ok","timestamp":1685658068102,"user":{"displayName":"amani bchir","userId":"17875292173569037577"},"user_tz":-60},"id":"F3r283wjTrUS","outputId":"4ba3291d-7042-403d-8df6-9db6a4ecf103"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow-docs\n","  Downloading tensorflow_docs-2023.5.24.56664-py3-none-any.whl (183 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/183.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.6/183.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-docs) (1.4.0)\n","Collecting astor (from tensorflow-docs)\n","  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-docs) (3.1.2)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from tensorflow-docs) (5.8.0)\n","Requirement already satisfied: protobuf>=3.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow-docs) (3.20.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from tensorflow-docs) (6.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->tensorflow-docs) (2.1.2)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->tensorflow-docs) (2.16.3)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->tensorflow-docs) (4.3.3)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from nbformat->tensorflow-docs) (5.3.0)\n","Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbformat->tensorflow-docs) (5.7.1)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs) (23.1.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs) (0.19.3)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->nbformat->tensorflow-docs) (3.3.0)\n","Installing collected packages: astor, tensorflow-docs\n","Successfully installed astor-0.8.1 tensorflow-docs-2023.5.24.56664\n"]}],"source":["!pip install tensorflow-docs"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"hBi-YHTyTf8N","executionInfo":{"status":"ok","timestamp":1685658074027,"user_tz":-60,"elapsed":5938,"user":{"displayName":"amani bchir","userId":"17875292173569037577"}}},"outputs":[],"source":["from tensorflow_docs.vis import embed\n","from tensorflow import keras\n","from imutils import paths\n","\n","import matplotlib.pyplot as plt\n","import imageio\n"," \n","\n","import numpy as np\n","import cv2\n","import os\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n","from tensorflow.keras.models import Model\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, f1_score"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"z8a_kfrLTj5J","executionInfo":{"status":"ok","timestamp":1685658074028,"user_tz":-60,"elapsed":13,"user":{"displayName":"amani bchir","userId":"17875292173569037577"}}},"outputs":[],"source":["IMG_SIZE = 224\n","BATCH_SIZE = 64\n","EPOCHS = 10\n","\n","MAX_SEQ_LENGTH = 20\n","NUM_FEATURES = 2048"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":999,"status":"ok","timestamp":1685658075015,"user":{"displayName":"amani bchir","userId":"17875292173569037577"},"user_tz":-60},"id":"0WTkJdmZTyOh","outputId":"9dd4e884-9d30-488a-8642-976ae2231dc4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total videos for validating: 32\n","Total videos for testing: 33\n"]}],"source":["df = pd.read_csv(\"/content/drive/MyDrive/PFA/videos/video_train.csv\")\n","val_df = pd.read_csv(\"/content/drive/MyDrive/PFA/videos/video_val.csv\")\n","test_df = pd.read_csv(\"/content/drive/MyDrive/PFA/videos/video_test.csv\")\n","\n","print(f\"Total videos for validating: {len(val_df)}\")\n","print(f\"Total videos for testing: {len(test_df)}\")\n","\n"]},{"cell_type":"code","source":["# Define paths to video files and corresponding labels\n","video_paths = df[\"video_path\"].tolist()\n","labels = df[\"F1\"].tolist()\n","\n","# Define parameters\n","image_size = (224, 224)  # Size of input frames\n","num_classes = len(set(labels))  # Number of distinct classes in your dataset\n","\n","# Extract video frames and preprocess\n","def extract_frames(video_path):\n","    frames = []\n","    cap = cv2.VideoCapture(video_path)\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, image_size)\n","        frame = frame / 255.0  # Normalize pixel values between 0 and 1\n","        frames.append(frame)\n","    cap.release()\n","    return frames\n","\n","# Extract frames for all videos\n","all_frames = [extract_frames(path) for path in video_paths]\n","\n","# Split data into training and testing sets\n","train_frames, test_frames, train_labels, test_labels = train_test_split(\n","    all_frames, labels, test_size=0.2, random_state=42)\n","\n","# Convert frames to numpy arrays\n","train_frames = np.array(train_frames)\n","test_frames = np.array(test_frames)\n","\n","# Load pretrained CNN model\n","base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n","\n","# Add classification head\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(128, activation='relu')(x)\n","predictions = Dense(num_classes, activation='softmax')(x)\n","model = Model(inputs=base_model.input, outputs=predictions)\n","\n","# Freeze base model layers\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(train_frames, train_labels, epochs=10, batch_size=16)\n","\n","# Evaluate the model\n","test_loss, test_accuracy = model.evaluate(test_frames, test_labels)\n","print(\"Test Loss:\", test_loss)\n","print(\"Test Accuracy:\", test_accuracy)\n","\n","# Evaluate the model\n","test_predictions = np.argmax(model.predict(test_frames), axis=1)\n","f1 = f1_score(test_labels, test_predictions, average='weighted')\n","print(\"F1 Score:\", f1)\n","\n","# Make predictions on new videos\n","new_video_paths = [\"path/to/new/video1.mp4\", \"path/to/new/video2.mp4\"]  # Paths to new videos\n","new_frames = [extract_frames(path) for path in new_video_paths]\n","new_frames = np.array(new_frames)\n","predictions = model.predict(new_frames)\n","predicted_labels = np.argmax(predictions, axis=1)\n","\n","# Print predicted labels\n","for path, label in zip(new_video_paths, predicted_labels):\n","    print(f\"Video: {path} - Predicted Label: {label}\")\n","\n","# Generate classification report\n","classification_rep = classification_report(test_labels, test_predictions)\n","print(classification_rep)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":504},"id":"Cuk-2tRHRP0F","executionInfo":{"status":"error","timestamp":1685658167304,"user_tz":-60,"elapsed":92293,"user":{"displayName":"amani bchir","userId":"17875292173569037577"}},"outputId":"ffb5ac3c-d34c-463e-9f80-542ce5acf645"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-c56feddb2bd1>:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  train_frames = np.array(train_frames)\n","<ipython-input-6-c56feddb2bd1>:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  test_frames = np.array(test_frames)\n"]},{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n","9406464/9406464 [==============================] - 0s 0us/step\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-c56feddb2bd1>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1083\u001b[0m             \"Failed to find data adapter that can handle input: {}, {}\".format(\n\u001b[1;32m   1084\u001b[0m                 \u001b[0m_type_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_type_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'str'>\"})"]}]},{"cell_type":"code","source":[],"metadata":{"id":"fmWWMN60R02_"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}