{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"h1xJhIUTsQQS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","!pip install transformers\n"],"metadata":{"id":"4fmA1sNO1C0_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dXAzmZ3ooZWa"},"outputs":[],"source":["import transformers\n","from sklearn.metrics import classification_report, confusion_matrix\n","import pandas as pd\n","import tensorflow as tf\n","import pandas as pd\n","import string\n","import re\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from transformers import BertForSequenceClassification\n","\n","from transformers import TFBertModel\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","import numpy as np"]},{"cell_type":"code","source":["train_df = pd.read_csv(\"/content/drive/MyDrive/PFA/DATASET/trainset.csv\",encoding='ISO-8859-1')\n","valid_df = pd.read_csv(\"/content/drive/MyDrive/PFA/DATASET/validationset.csv\",encoding='ISO-8859-1')\n","test_df =pd.read_csv(\"/content/drive/MyDrive/PFA/DATASET/testset.csv\",encoding='ISO-8859-1')"],"metadata":{"id":"sWNJ3RFnsYxd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_captions(df,output):\n","  individual_captions = pd.DataFrame(columns=[\"caption\",\"I-E\",\"S-N\",\"T-F\",\"J-P\"])\n","\n","  for index,row in df.iterrows():\n","    for caption_index,caption in enumerate(eval(row['text_clean'])):\n","      # Define the new row to be added\n","      new_row = {'caption': caption.strip(\"\\n\"),\"I-E\":df[\"I-E\"][index],\\\n","                 'S-N':df[\"S-N\"][index],'T-F':df[\"T-F\"][index],'J-P':df[\"J-P\"][index]}\n","      # Use the loc method to add the new row to the DataFrame\n","      individual_captions.loc[len(individual_captions)] = new_row\n","  individual_captions =individual_captions.drop(individual_captions[individual_captions['caption']==''].index)\n","  individual_captions.to_csv(output)\n","  return individual_captions\n","  "],"metadata":{"id":"1kaHi6UPsPwX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df_indiv = extract_captions(train_df,\"/content/drive/MyDrive/PFA/DATASET/indiv_trainset.csv\")"],"metadata":{"id":"e9bz5tJ4tKb-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df_indiv.head()"],"metadata":{"id":"BgHaCHA45E1c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["valid_df_indiv  = extract_captions(valid_df,\"/content/drive/MyDrive/PFA/DATASET/indiv_validationset.csv\")"],"metadata":{"id":"dTVXOmC2tT5g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df_indiv  = extract_captions(test_df,\"/content/drive/MyDrive/PFA/DATASET/indiv_testset.csv\")"],"metadata":{"id":"lgb6_uHAtUD4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased',do_lower_case =True)"],"metadata":{"id":"HoAOSfWGuRqN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def encode_caption(caption):\n","    tokens = tokenizer.encode_plus(\n","        caption,\n","        add_special_tokens=True,\n","        max_length=128,\n","        padding=True,truncation=True,\n","        return_attention_mask=True\n","    )\n","    return tokens['input_ids'], tokens['attention_mask']\n","\n","\n","def encode_individual_captions(train_df_indiv ,val_df_indiv ,test_df_indiv ):\n","  train_df_indiv [['input_ids', 'attention_mask']] = train_df_indiv ['caption'].apply(lambda x: pd.Series(encode_caption(x)))\n","  valid_df_indiv [['input_ids', 'attention_mask']] = valid_df_indiv ['caption'].apply(lambda x: pd.Series(encode_caption(x)))\n","  test_df_indiv [['input_ids', 'attention_mask']] = test_df_indiv ['caption'].apply(lambda x: pd.Series(encode_caption(x)))\n","  return train_df_indiv, valid_df_indiv, test_df_indiv\n"],"metadata":{"id":"LIkp3ZVVtKRW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df_indiv, valid_df_indiv, test_df_indiv = encode_individual_captions(train_df_indiv ,valid_df_indiv ,test_df_indiv )"],"metadata":{"id":"WaAT5nJ7uOSk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train = train_df_indiv[['input_ids','attention_mask']]\n","X_valid = valid_df_indiv[['input_ids','attention_mask']]\n","X_test = test_df_indiv[['input_ids','attention_mask']]"],"metadata":{"id":"lKAHqEF65DCB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train_ie= train_df_indiv[\"I-E\"]\n","y_train_sn= train_df_indiv[\"S-N\"]\n","y_train_tf= train_df_indiv[\"T-F\"]\n","y_train_jp= train_df_indiv[\"J-P\"]\n","\n","y_valid_ie= valid_df_indiv[\"I-E\"]\n","y_valid_sn= valid_df_indiv[\"S-N\"]\n","y_valid_tf= valid_df_indiv[\"T-F\"]\n","y_valid_jp= valid_df_indiv[\"J-P\"]\n","\n","y_test_ie= test_df_indiv[\"I-E\"]\n","y_test_sn= test_df_indiv[\"S-N\"]\n","y_test_tf= test_df_indiv[\"T-F\"]\n","y_test_jp= test_df_indiv[\"J-P\"]"],"metadata":{"id":"RiFvwpSa4wpu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_input_ids_ragged = tf.ragged.constant(X_train['input_ids'].tolist())\n","train_attention_mask_ragged = tf.ragged.constant(X_train['attention_mask'].tolist())\n","\n","val_input_ids_ragged = tf.ragged.constant(X_valid['input_ids'].tolist())\n","val_attention_mask_ragged = tf.ragged.constant(X_valid['attention_mask'].tolist())\n","\n","test_input_ids_ragged = tf.ragged.constant(X_test['input_ids'].tolist())\n","test_attention_mask_ragged = tf.ragged.constant(X_test['attention_mask'].tolist())\n"],"metadata":{"id":"Mc5ALlVU-pyw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# convert the ragged tensors to dense tensors\n","train_input_ids = train_input_ids_ragged.to_tensor()\n","train_attention_mask = train_attention_mask_ragged.to_tensor()\n","\n","val_input_ids = val_input_ids_ragged.to_tensor()\n","val_attention_mask = val_attention_mask_ragged.to_tensor()\n","\n","test_input_ids = test_input_ids_ragged.to_tensor()\n","test_attention_mask = test_attention_mask_ragged.to_tensor()\n"],"metadata":{"id":"I1cccSEXw7hz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\",\n","    num_labels=2\n",")"],"metadata":{"id":"k2X-tYIJw-3a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MAX_LEN = 128"],"metadata":{"id":"4Hn6GOT_7WJG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_model(bert_model, max_len=MAX_LEN): \n","  def recall_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","\n","  def precision_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","  def f1_m(y_true, y_pred):\n","    precision = precision_m(y_true, y_pred)\n","    recall = recall_m(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n","  input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n","  attention_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n","  output = tf.keras.layers.Dense(1, activation='sigmoid')(bert_model([input_ids, attention_mask])[1])\n","  model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n","\n","  \n","  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])\n","  \n","  return model"],"metadata":{"id":"dZQYQ4E0xE04"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = create_model(bert_model, MAX_LEN)"],"metadata":{"id":"YXy0hyr4xIYT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"y3ug4naFxI3K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" history_bert_ie = model.fit(\n","    [train_input_ids, train_attention_mask], y_train_ie, \n","\n","    validation_data=([val_input_ids, val_attention_mask], y_valid_ie),\n","\n","    epochs=10, batch_size=32)\n"," loss, accuracy, f1, precision, recall = model.evaluate([test_input_ids, test_attention_mask], y_test_ie)\n","    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n","    print(f\"Test f1: { f1 }\")"],"metadata":{"id":"ShmKIf3IxKfW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred_ie = model.predict([test_input_ids, test_atention_mask])\n","predictions_ie = np.round(y_pred_ie).astype(int)"],"metadata":{"id":"5oiLTkrmxMN7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('\\tClassification Report for BERT FOR I-E:\\n\\n',classification_report(y_test_ie,predictions_ie, target_names=['0', '1']))"],"metadata":{"id":"KFgBtxLQxPYF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cm = confusion_matrix(y_test_ie, predictions_ie)\n","sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.show()"],"metadata":{"id":"lVFr_6bLxPiM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Kl27oqHyxPke"},"execution_count":null,"outputs":[]}]}